{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5507d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Optimizer\n",
    "import timeit\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a117de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print(classname)\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == 'A':\n",
    "                \"\"\"\n",
    "                For CIFAR10 ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(lambda x:\n",
    "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            elif option == 'B':\n",
    "                self.shortcut = nn.Sequential(\n",
    "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                     nn.BatchNorm2d(self.expansion * planes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet20():\n",
    "    return ResNet(BasicBlock, [3, 3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d11abf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = resnet20()\n",
    "model.to(device)\n",
    "torch.save(model.state_dict(), \"cifar10_resnet.pth\")\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1a172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(root='./data', train=True, transform=transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32, 4),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]), download=True),\n",
    "    batch_size=128, shuffle=True,\n",
    "    num_workers=4, pin_memory=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(root='./data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])),\n",
    "    batch_size=64, shuffle=False,\n",
    "    num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857ebefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    \n",
    "    correct_pred = (y_pred_tags == y_test).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "    \n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc\n",
    "\n",
    "learning_rate_list = [0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.08, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f62fbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_data = []\n",
    "\n",
    "for learning_rate in learning_rate_list:\n",
    "\n",
    "    model = resnet20()\n",
    "    model.load_state_dict(torch.load(\"cifar10_resnet.pth\"))\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    timelist_adam = []\n",
    "    losslist_adam_train = []\n",
    "    acclist_adam_train = []\n",
    "    losslist_adam_test = []\n",
    "    acclist_adam_test = []\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    for e in range(1, 200+1):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(X_batch)\n",
    "\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            acc = multi_acc(y_pred, y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_time = timeit.default_timer()\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            val_epoch_loss = 0\n",
    "            val_epoch_acc = 0\n",
    "\n",
    "            model.eval()\n",
    "            for X_val_batch, y_val_batch in val_loader:\n",
    "                X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "\n",
    "                y_val_pred = model(X_val_batch)\n",
    "\n",
    "                val_loss = criterion(y_val_pred, y_val_batch)\n",
    "                val_acc = multi_acc(y_val_pred, y_val_batch)\n",
    "\n",
    "                val_epoch_loss += val_loss.item()\n",
    "                val_epoch_acc += val_acc.item()\n",
    "\n",
    "\n",
    "        if e % 50 == 0:\n",
    "            print(f'Epoch {e+0:03}: | train Loss: {epoch_loss/len(train_loader):.5f} | train Acc: {epoch_acc/len(train_loader):.3f}')\n",
    "            print(f'Epoch {e+0:03}: | test Loss: {val_epoch_loss/len(val_loader):.5f} | test Acc: {val_epoch_acc/len(val_loader):.3f}')\n",
    "\n",
    "\n",
    "        epoch_time = timeit.default_timer()\n",
    "        timelist_adam.append(epoch_time - start_time)\n",
    "        losslist_adam_train.append(epoch_loss/len(train_loader))\n",
    "        acclist_adam_train.append(epoch_acc/len(train_loader))\n",
    "        losslist_adam_test.append(val_epoch_loss/len(val_loader))\n",
    "        acclist_adam_test.append(val_epoch_acc/len(val_loader))\n",
    "    \n",
    "    adam_data.append(timelist_adam)\n",
    "    adam_data.append(losslist_adam_train)\n",
    "    adam_data.append(acclist_adam_train)\n",
    "    adam_data.append(losslist_adam_test)\n",
    "    adam_data.append(acclist_adam_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173966f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_data = []\n",
    "\n",
    "for learning_rate in learning_rate_list:\n",
    "\n",
    "    model = resnet20()\n",
    "    model.load_state_dict(torch.load(\"cifar10_resnet.pth\"))\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    timelist_sgd = []\n",
    "    losslist_sgd_train = []\n",
    "    acclist_sgd_train = []\n",
    "    losslist_sgd_test = []\n",
    "    acclist_sgd_test = []\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    for e in range(1, 200+1):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(X_batch)\n",
    "\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            acc = multi_acc(y_pred, y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_time = timeit.default_timer()\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            val_epoch_loss = 0\n",
    "            val_epoch_acc = 0\n",
    "\n",
    "            model.eval()\n",
    "            for X_val_batch, y_val_batch in val_loader:\n",
    "                X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "\n",
    "                y_val_pred = model(X_val_batch)\n",
    "\n",
    "                val_loss = criterion(y_val_pred, y_val_batch)\n",
    "                val_acc = multi_acc(y_val_pred, y_val_batch)\n",
    "\n",
    "                val_epoch_loss += val_loss.item()\n",
    "                val_epoch_acc += val_acc.item()\n",
    "\n",
    "\n",
    "        if e % 50 == 0:\n",
    "            print(f'Epoch {e+0:03}: | train Loss: {epoch_loss/len(train_loader):.5f} | train Acc: {epoch_acc/len(train_loader):.3f}')\n",
    "            print(f'Epoch {e+0:03}: | test Loss: {val_epoch_loss/len(val_loader):.5f} | test Acc: {val_epoch_acc/len(val_loader):.3f}')\n",
    "\n",
    "\n",
    "        epoch_time = timeit.default_timer()\n",
    "        timelist_sgd.append(epoch_time - start_time)\n",
    "        losslist_sgd_train.append(epoch_loss/len(train_loader))\n",
    "        acclist_sgd_train.append(epoch_acc/len(train_loader))\n",
    "        losslist_sgd_test.append(val_epoch_loss/len(val_loader))\n",
    "        acclist_sgd_test.append(val_epoch_acc/len(val_loader))\n",
    "    \n",
    "    sgd_data.append(timelist_sgd)\n",
    "    sgd_data.append(losslist_sgd_train)\n",
    "    sgd_data.append(acclist_sgd_train)\n",
    "    sgd_data.append(losslist_sgd_test)\n",
    "    sgd_data.append(acclist_sgd_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7276cec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_optimizer\n",
    "\n",
    "adahes_data = []\n",
    "\n",
    "for learning_rate in learning_rate_list:\n",
    "\n",
    "    model = resnet20()\n",
    "    model.load_state_dict(torch.load(\"cifar10_resnet.pth\"))\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch_optimizer.Adahessian(\n",
    "    model.parameters(),\n",
    "    lr= learning_rate,\n",
    "    betas= (0.9, 0.999),\n",
    "    eps= 1e-4,\n",
    "    weight_decay=0.0,\n",
    "    hessian_power=1.0,\n",
    "    )\n",
    "\n",
    "    timelist_adahes = []\n",
    "    losslist_adahes_train = []\n",
    "    acclist_adahes_train = []\n",
    "    losslist_adahes_test = []\n",
    "    acclist_adahes_test = []\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    for e in range(1, 200+1):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(X_batch)\n",
    "\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            acc = multi_acc(y_pred, y_batch)\n",
    "\n",
    "            loss.backward(create_graph = True)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            val_epoch_loss = 0\n",
    "            val_epoch_acc = 0\n",
    "\n",
    "            model.eval()\n",
    "            for X_val_batch, y_val_batch in val_loader:\n",
    "                X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "\n",
    "                y_val_pred = model(X_val_batch)\n",
    "\n",
    "                val_loss = criterion(y_val_pred, y_val_batch)\n",
    "                val_acc = multi_acc(y_val_pred, y_val_batch)\n",
    "\n",
    "                val_epoch_loss += val_loss.item()\n",
    "                val_epoch_acc += val_acc.item()\n",
    "\n",
    "\n",
    "        if e % 50 == 0:\n",
    "            print(f'Epoch {e+0:03}: | train Loss: {epoch_loss/len(train_loader):.5f} | train Acc: {epoch_acc/len(train_loader):.3f}')\n",
    "            print(f'Epoch {e+0:03}: | test Loss: {val_epoch_loss/len(val_loader):.5f} | test Acc: {val_epoch_acc/len(val_loader):.3f}')\n",
    "\n",
    "\n",
    "        epoch_time = timeit.default_timer()\n",
    "        losslist_adahes_train.append(epoch_loss/len(train_loader))\n",
    "        timelist_adahes.append(epoch_time - start_time)\n",
    "        acclist_adahes_train.append(epoch_acc/len(train_loader))\n",
    "        losslist_adahes_test.append(val_epoch_loss/len(val_loader))\n",
    "        acclist_adahes_test.append(val_epoch_acc/len(val_loader))\n",
    "    \n",
    "    adahes_data.append(timelist_adahes)\n",
    "    adahes_data.append(losslist_adahes_train)\n",
    "    adahes_data.append(acclist_adahes_train)\n",
    "    adahes_data.append(losslist_adahes_test)\n",
    "    adahes_data.append(acclist_adahes_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d34b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbfgs_data = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rate_list:\n",
    "\n",
    "    model = resnet20()\n",
    "    model.load_state_dict(torch.load(\"cifar10_resnet.pth\"))\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.LBFGS(model.parameters(), lr = learning_rate, history_size=10, line_search_fn='strong_wolfe')\n",
    "\n",
    "    timelist_lbfgs = []\n",
    "    losslist_lbfgs_test = []\n",
    "    acclist_lbfgs_test = []\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    for e in range(1, 200+1):\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = model(X_batch)\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "            optimizer.step(closure)\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            val_epoch_loss = 0\n",
    "            val_epoch_acc = 0\n",
    "\n",
    "            model.eval()\n",
    "            for X_val_batch, y_val_batch in val_loader:\n",
    "                X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "\n",
    "                y_val_pred = model(X_val_batch)\n",
    "\n",
    "                val_loss = criterion(y_val_pred, y_val_batch)\n",
    "                val_acc = multi_acc(y_val_pred, y_val_batch)\n",
    "\n",
    "                val_epoch_loss += val_loss.item()\n",
    "                val_epoch_acc += val_acc.item()\n",
    "\n",
    "\n",
    "        if e % 50 == 0:\n",
    "             print(f'Epoch {e+0:03}: | test Loss: {val_epoch_loss/len(val_loader):.5f} | test Acc: {val_epoch_acc/len(val_loader):.3f}')\n",
    "\n",
    "        epoch_time = timeit.default_timer()\n",
    "        timelist_lbfgs.append(epoch_time - start_time)\n",
    "        losslist_lbfgs_test.append(val_epoch_loss/len(val_loader))\n",
    "        acclist_lbfgs_test.append(val_epoch_acc/len(val_loader))\n",
    "    \n",
    "    lbfgs_data.append(timelist_lbfgs)\n",
    "    lbfgs_data.append(losslist_lbfgs_test)\n",
    "    lbfgs_data.append(acclist_lbfgs_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b94c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_product(xs, ys):\n",
    "    \n",
    "    return sum([torch.sum(x * y) for (x, y) in zip(xs, ys)])\n",
    "\n",
    "def normalization(v):\n",
    "    # normalize a vector\n",
    "    \n",
    "    s = group_product(v, v)\n",
    "    s = s**0.5\n",
    "    s = s.cpu().item()\n",
    "    v = [vi / (s + 1e-6) for vi in v]\n",
    "    return v\n",
    "\n",
    "class NysHessianpartial():\n",
    "    \n",
    "    def __init__(self, rank, rho):\n",
    "        self.rank = rank\n",
    "        # rho is the regularization in Nystrom sketch\n",
    "        self.rho = rho\n",
    "    \n",
    "    def get_params_grad(self, model):\n",
    "        # get parameters and differentiation\n",
    "        params = []\n",
    "        grads = []\n",
    "        for param in model.parameters():\n",
    "            if not param.requires_grad:\n",
    "                continue\n",
    "            params.append(param)\n",
    "            grads.append(0. if param.grad is None else param.grad + 0.)\n",
    "        return params, grads\n",
    "    \n",
    "    def update_Hessian(self, X_batch, y_batch, model, criterion, device):\n",
    "        \n",
    "        shift = 0.001\n",
    "        # get the model parameters and gradients\n",
    "        params, gradsH = self.get_params_grad(model)\n",
    "        # remember the size for each group of parameters\n",
    "        self.size_vec = [p.size() for p in params]\n",
    "        # store random gaussian vector to a matrix\n",
    "        test_matrix = []\n",
    "        # Hessian vector product\n",
    "        hv_matrix = []\n",
    "        \n",
    "        for i in range(self.rank):\n",
    "            # generate gaussian random vector\n",
    "            v = [torch.randn(p.size()).to(device) for p in params]\n",
    "            # normalize\n",
    "            v = normalization(v)\n",
    "            # zero vector to store the shape\n",
    "            hv_add = [torch.zeros(p.size()).to(device) for p in params]\n",
    "        \n",
    "            # update hessian with a subsample batch\n",
    "            \n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            model.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward(create_graph=True)\n",
    "            params, gradsH = self.get_params_grad(model)\n",
    "            # calculate the Hessian vector product\n",
    "            hv = torch.autograd.grad(gradsH, params, grad_outputs=v,only_inputs=True,retain_graph=True)\n",
    "            # add initial shift\n",
    "            for i in range(len(hv)):\n",
    "                hv_add[i].data = hv[i].data.add_(hv_add[i].data)    \n",
    "                hv_add[i].data = hv_add[i].data.add_(v[i].data * torch.tensor(shift)) \n",
    "            \n",
    "            # reshape the Hessian vector product into a long vector\n",
    "            hv_ex = torch.cat([gi.reshape(-1) for gi in hv_add])\n",
    "            # reshape the random vector into a long vector\n",
    "            test_ex = torch.cat([gi.view(-1) for gi in v])\n",
    "            \n",
    "            # append long vectors into a large matrix\n",
    "            hv_matrix.append(hv_ex)\n",
    "            test_matrix.append(test_ex)\n",
    "        \n",
    "        # assemble the large matrix\n",
    "        hv_matrix_ex = torch.column_stack(hv_matrix)\n",
    "        test_matrix_ex = torch.column_stack(test_matrix)\n",
    "        # calculate Omega^T * A * Omega for Cholesky\n",
    "        choleskytarget = torch.mm(test_matrix_ex.t(), hv_matrix_ex)\n",
    "        # perform Cholesky, if fails, do eigendecomposition\n",
    "        # the new shift is the abs of smallest eigenvalue (negative) plus the original shift\n",
    "        try:\n",
    "            C_ex = torch.linalg.cholesky(choleskytarget)\n",
    "        except:\n",
    "            # eigendecomposition, eigenvalues and eigenvector matrix\n",
    "            eigs, eigvectors = torch.linalg.eigh(choleskytarget)\n",
    "            shift = shift + torch.abs(torch.min(eigs))\n",
    "            # add shift to eigenvalues\n",
    "            eigs = eigs + shift\n",
    "            # put back the matrix for Cholesky by eigenvector * eigenvalues after shift * eigenvector^T \n",
    "            C_ex = torch.linalg.cholesky(torch.mm(eigvectors, torch.mm(torch.diag(eigs), eigvectors.T)))\n",
    "        \n",
    "        # triangular solve\n",
    "        # B_ex = torch.linalg.solve_triangular(C_ex, hv_matrix_ex, upper = False, left = False)\n",
    "        B_ex = torch.triangular_solve(hv_matrix_ex.t(), C_ex.t(), upper = True)\n",
    "        # SVD\n",
    "        # U, S, V = torch.linalg.svd(B_ex, full_matrices = False)\n",
    "        U, S, V = torch.linalg.svd(B_ex[0].t(), full_matrices = False)\n",
    "        self.U = U\n",
    "        self.S = torch.max(torch.square(S) - torch.tensor(shift), torch.tensor(0.0))\n",
    "\n",
    "class NysHessianOpt(Optimizer):\n",
    "    r\"\"\"Implements NysHessian.\n",
    "    Args:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float): learning rate\n",
    "        rank (int): sketch rank\n",
    "        rho: regularization\n",
    "    \"\"\"\n",
    "    def __init__(self, params, rank = 100, rho = 0.1):\n",
    "        # initialize the optimizer    \n",
    "        defaults = dict(rank = rank, rho = rho)\n",
    "        self.nysh = NysHessianpartial(rank, rho)\n",
    "        super(NysHessianOpt, self).__init__(params, defaults)\n",
    "         \n",
    "    def step(self, lr):\n",
    "        # one step update\n",
    "        for group in self.param_groups:\n",
    "            rho = group['rho']\n",
    "            # compute gradient as a long vector\n",
    "            g = torch.cat([p.grad.view(-1) for p in group['params']])\n",
    "            # calculate the search direction by Nystrom sketch and solve\n",
    "            UTg = torch.mv(self.nysh.U.t(), g) \n",
    "            g_new = torch.mv(self.nysh.U, (self.nysh.S + rho).reciprocal() * UTg) + g / rho - torch.mv(self.nysh.U, UTg) / rho            \n",
    "            ls = 0\n",
    "            # update model parameters\n",
    "            for p in group['params']:\n",
    "                gp = g_new[ls:ls+torch.numel(p)].view(p.shape)\n",
    "                ls += torch.numel(p)\n",
    "                p.data.add_(-lr * gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2abeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "skechysgd_data = []\n",
    "\n",
    "hes_interval = 2 * len(train_loader) - 1\n",
    "# update Hessian and Nystrom sketch every couple of steps\n",
    "\n",
    "for learning_rate in learning_rate_list:\n",
    "    model = resnet20()\n",
    "    model.load_state_dict(torch.load(\"cifar10_resnet.pth\"))\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = NysHessianOpt(model.parameters())\n",
    "\n",
    "    hes_iter = 0\n",
    "\n",
    "    timelist_skechysgd = []\n",
    "    losslist_skechysgd_train = []\n",
    "    acclist_skechysgd_train = []\n",
    "    losslist_skechysgd_test = []\n",
    "    acclist_skechysgd_test = []\n",
    "\n",
    "    lr = torch.tensor(learning_rate)\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "\n",
    "    for e in range(1, 200+1):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "\n",
    "            if hes_iter % hes_interval == 0:\n",
    "                # update Hessian and sketch\n",
    "                optimizer.nysh.update_Hessian(X_batch, y_batch, model, criterion, device)\n",
    "\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(X_batch)\n",
    "\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            acc = multi_acc(y_pred, y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step(lr)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            hes_iter += 1\n",
    "            epoch_time = timeit.default_timer()\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            val_epoch_loss = 0\n",
    "            val_epoch_acc = 0\n",
    "\n",
    "            model.eval()\n",
    "            for X_val_batch, y_val_batch in val_loader:\n",
    "                X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "\n",
    "                y_val_pred = model(X_val_batch)\n",
    "\n",
    "                val_loss = criterion(y_val_pred, y_val_batch)\n",
    "                val_acc = multi_acc(y_val_pred, y_val_batch)\n",
    "\n",
    "                val_epoch_loss += val_loss.item()\n",
    "                val_epoch_acc += val_acc.item()\n",
    "\n",
    "\n",
    "        if e % 50 == 0:\n",
    "            print(f'Epoch {e+0:03}: | train Loss: {epoch_loss/len(train_loader):.5f} | train Acc: {epoch_acc/len(train_loader):.3f}')\n",
    "            print(f'Epoch {e+0:03}: | test Loss: {val_epoch_loss/len(val_loader):.5f} | test Acc: {val_epoch_acc/len(val_loader):.3f}')\n",
    "\n",
    "\n",
    "        epoch_time = timeit.default_timer()\n",
    "        timelist_skechysgd.append(epoch_time - start_time)\n",
    "        losslist_skechysgd_train.append(epoch_loss/len(train_loader))\n",
    "        acclist_skechysgd_train.append(epoch_acc/len(train_loader))\n",
    "        losslist_skechysgd_test.append(val_epoch_loss/len(val_loader))\n",
    "        acclist_skechysgd_test.append(val_epoch_acc/len(val_loader))\n",
    "    \n",
    "    skechysgd_data.append(timelist_skechysgd)\n",
    "    skechysgd_data.append(losslist_skechysgd_train)\n",
    "    skechysgd_data.append(acclist_skechysgd_train)\n",
    "    skechysgd_data.append(losslist_skechysgd_test)\n",
    "    skechysgd_data.append(acclist_skechysgd_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6612f662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_product(xs, ys):\n",
    "    \"\"\"\n",
    "    the inner product of two lists of variables xs,ys\n",
    "    :param xs:\n",
    "    :param ys:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return sum([torch.sum(x*y) for (x, y) in zip(xs, ys)])\n",
    "\n",
    "def group_add(params, update, alpha=1):\n",
    "    \"\"\"\n",
    "    params = params + update*alpha\n",
    "    :param params: list of variable\n",
    "    :param update: list of data\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for i,p in enumerate(params):\n",
    "        params[i].data.add_(update[i]*alpha) \n",
    "    return params\n",
    "\n",
    "def get_params_grad(model):\n",
    "        # get parameters and differentiation\n",
    "        params = []\n",
    "        grads = []\n",
    "        for param in model.parameters():\n",
    "            if not param.requires_grad:\n",
    "                continue\n",
    "            params.append(param)\n",
    "            grads.append(0. if param.grad is None else param.grad + 0.)\n",
    "        return params, grads\n",
    "\n",
    "class NewtonCG(Optimizer):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        # initialize the optimizer    \n",
    "        defaults = dict()\n",
    "        super(NewtonCG, self).__init__(params, defaults)\n",
    "        \n",
    "    def cg_step(self, g, gradsH, cg_iter, tol):\n",
    "        gnorms = group_product(g, g)\n",
    "        params = self.param_groups[0]['params']\n",
    "        weight = 0.0\n",
    "        zs = [0.0*p.data for p in params]\n",
    "        rs = [g.data + weight*p.data for g,p in zip(g, params)]\n",
    "        ds = [g.data - weight*p.data for g,p in zip(g, params)]\n",
    "        \n",
    "        for i in range(cg_iter):\n",
    "            if gnorms <= tol:\n",
    "                return zs\n",
    "            if i != 0:\n",
    "                ratio = gnorms / gnorms_prev\n",
    "                ds = [rsd.data + ratio * dsd.data for rsd, dsd in zip(rs, ds)]\n",
    "            hv = torch.autograd.grad(gradsH, params, grad_outputs=ds,only_inputs=True,retain_graph=True)\n",
    "            alpha = gnorms / group_product(ds, hv)\n",
    "            zs = [zsd.data + alpha * dsd.data for zsd, dsd in zip(zs, ds)]\n",
    "            rs = [rsd.data - alpha * hvd.data for rsd, hvd in zip(rs, hv)]\n",
    "            gnorms_prev = gnorms\n",
    "            gnorms = group_product(rs, rs)\n",
    "        \n",
    "        return zs\n",
    "         \n",
    "    def step(self, lr, gradsH, cg_iter, tol):\n",
    "        # one step update\n",
    "        for group in self.param_groups:\n",
    "            g = [p.grad for p in group['params']]\n",
    "            g_new = self.cg_step(g, gradsH, cg_iter, tol)\n",
    "            # update model parameters\n",
    "            for p, g_newgroup in zip(group['params'], g_new):\n",
    "                p.data.add_(-lr * g_newgroup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c182a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(root='./data', train=True, transform=transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32, 4),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]), download=True),\n",
    "    batch_size=128, shuffle=True,\n",
    "    num_workers=4, pin_memory=True)\n",
    "\n",
    "train_full_data = torch.randn(50000, 3, 32, 32)\n",
    "train_full_label = torch.zeros(50000).type(torch.LongTensor)\n",
    "\n",
    "for i, (d, l) in enumerate(trainset):\n",
    "    train_full_data[128*i:128*(i+1), :] = d\n",
    "    train_full_label[128*i:128*(i+1)] = l\n",
    "\n",
    "n = len(train_loader.dataset)\n",
    "batch_size = train_loader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d156fc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "newtoncg_data = []\n",
    "\n",
    "for learning_rate in learning_rate_list:\n",
    "\n",
    "    model = resnet20()\n",
    "    model.load_state_dict(torch.load(\"cifar10_resnet.pth\"))\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = NewtonCG(model.parameters())\n",
    "\n",
    "    timelist_newtoncg = []\n",
    "    losslist_newtoncg_test = []\n",
    "    acclist_newtoncg_test = []\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    tol = 1e-6\n",
    "    cg_iter = 10\n",
    "    lr = torch.tensor(learning_rate)\n",
    "    cgupdate = 0\n",
    "\n",
    "    for e in range(1, 100+1):\n",
    "\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            hess_data = train_full_data[cgupdate * 128:(cgupdate + 1)*128, :].to(device)\n",
    "            hess_label = train_full_label[cgupdate * 128:(cgupdate + 1)*128].to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "            outputs_h = model(hess_data)\n",
    "            loss = criterion(outputs_h, hess_label)\n",
    "            loss.backward(create_graph=True)\n",
    "            params, gradsH = get_params_grad(model)\n",
    "\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step(lr, gradsH, cg_iter, tol)\n",
    "            cgupdate += 1\n",
    "            cgupdate = cgupdate % len(trainset)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            val_epoch_loss = 0\n",
    "            val_epoch_acc = 0\n",
    "\n",
    "            model.eval()\n",
    "            for X_val_batch, y_val_batch in val_loader:\n",
    "                X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "\n",
    "                y_val_pred = model(X_val_batch)\n",
    "\n",
    "                val_loss = criterion(y_val_pred, y_val_batch)\n",
    "                val_acc = multi_acc(y_val_pred, y_val_batch)\n",
    "\n",
    "                val_epoch_loss += val_loss.item()\n",
    "                val_epoch_acc += val_acc.item()\n",
    "\n",
    "        if e % 50 == 0:\n",
    "            print(f'Epoch {e+0:03}: | test Loss: {val_epoch_loss/len(val_loader):.5f} | test Acc: {val_epoch_acc/len(val_loader):.3f}')\n",
    "\n",
    "        epoch_time = timeit.default_timer()\n",
    "        timelist_newtoncg.append(epoch_time - start_time)\n",
    "        losslist_newtoncg_test.append(val_epoch_loss/len(val_loader))\n",
    "        acclist_newtoncg_test.append(val_epoch_acc/len(val_loader))\n",
    "    \n",
    "    newtoncg_data.append(timelist_newtoncg)\n",
    "    newtoncg_data.append(losslist_newtoncg_test)\n",
    "    newtoncg_data.append(acclist_newtoncg_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
