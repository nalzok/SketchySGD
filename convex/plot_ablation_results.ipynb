{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ablation_results(model_type, dataset, hyperparam):\n",
    "    results = {}\n",
    "    results_dir = os.path.join('./sketchysgd_'+hyperparam+'_ablation', model_type, dataset)\n",
    "    filenames = os.listdir(results_dir)\n",
    "\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.csv'):\n",
    "            if hyperparam == 'update_freq':\n",
    "                hyperparam_val = filename.split('_')[2] \n",
    "            elif hyperparam == 'rank':\n",
    "                hyperparam_val = filename.split('_')[1]\n",
    "                \n",
    "            df = pd.read_csv(os.path.join(results_dir, filename))\n",
    "            if hyperparam_val not in list(results.keys()):\n",
    "                results[hyperparam_val] = []\n",
    "            results[hyperparam_val].append(df)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_averages(opt_results):\n",
    "    averages = {}\n",
    "\n",
    "    for key in opt_results.keys():\n",
    "        averages[key] = {}\n",
    "        n = len(opt_results[key]) # Number of runs for a given hyperparameter\n",
    "        data_cols = list(opt_results[key][0].columns[1:]) # Get column names\n",
    "        for metric in data_cols:\n",
    "            for i in range(n):\n",
    "                if metric not in averages[key].keys():\n",
    "                    averages[key][metric] = 1/n * opt_results[key][i][metric].to_numpy()\n",
    "                else:\n",
    "                    averages[key][metric] += 1/n * opt_results[key][i][metric].to_numpy()\n",
    "\n",
    "    return averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_metric(avg_results, hyperparam, metric, dataset, lims):\n",
    "    plt.figure()\n",
    "\n",
    "    keys = list(avg_results.keys())\n",
    "    keys_numeric = []\n",
    "    for key in keys:\n",
    "        if key != 'infty':\n",
    "            keys_numeric.append(float(key))\n",
    "        else:\n",
    "            keys_numeric.append(math.inf)\n",
    "\n",
    "    keys_sorted = [x for _, x in sorted(zip(keys_numeric, keys))]\n",
    "\n",
    "    for hyperparam_val in keys_sorted:\n",
    "        if hyperparam_val == 'infty':\n",
    "            label = hyperparam['label'] + ' = ' + r'$\\infty$'\n",
    "        else:\n",
    "            label = f\"{hyperparam['label']} = {f'{float(hyperparam_val):.2f}'.rstrip('0').rstrip('.')}\"\n",
    "        if 'loss' in metric['name']:\n",
    "            plt.semilogy(np.cumsum(avg_results[hyperparam_val]['times']), avg_results[hyperparam_val][metric['name']], label = label)\n",
    "        else:\n",
    "            plt.plot(np.cumsum(avg_results[hyperparam_val]['times']), avg_results[hyperparam_val][metric['name']], label = label)\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel(metric['label'])\n",
    "    plt.title(dataset)\n",
    "    plt.ylim(lims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_multi_metric(model_type, dataset, hyperparam, metrics, lims):\n",
    "    ablation_results = load_ablation_results(model_type, dataset, hyperparam['name'])\n",
    "    avg_results = compute_averages(ablation_results)\n",
    "\n",
    "    for metric, lim in zip(metrics, lims):\n",
    "        plot_results_metric(avg_results, hyperparam, metric, dataset, lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_type(dataset):\n",
    "    if dataset in ['rcv1', 'news20', 'real-sim']:\n",
    "        model_type = 'logistic'\n",
    "    elif dataset in ['yearmsd', 'e2006', 'w8a']:\n",
    "        model_type = 'least_squares'\n",
    "\n",
    "    return model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [{'name': 'test_acc', 'label': 'Test Accuracy (%)'},\n",
    "           {'name': 'train_acc', 'label': 'Train Accuracy (%)'},\n",
    "           {'name': 'test_loss', 'label': 'Test Loss'},\n",
    "           {'name': 'train_loss', 'label': 'Train Loss'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'rcv1'\n",
    "model_type = get_model_type(dataset)\n",
    "hyperparam_uf = {'name': 'update_freq', 'label': 'Update frequency'}\n",
    "hyperparam_rk = {'name': 'rank', 'label': 'Rank'}\n",
    "\n",
    "# plot_results_multi_metric(model_type, dataset, hyperparam_uf, metrics, [[94, 96.5], [96, 100], [1e-1, 2e-1], [2e-2, 2e-1]])\n",
    "# plot_results_multi_metric(model_type, dataset, hyperparam_rk, metrics, [[94, 96.5], [96, 100], [1e-1, 2e-1], [2e-2, 2e-1]])\n",
    "\n",
    "metrics = [{'name': 'test_acc', 'label': 'Test Accuracy (%)'}]\n",
    "plot_results_multi_metric(model_type, dataset, hyperparam_uf, metrics, [[94, 96.5]])\n",
    "plt.savefig('rcv1_uf.pdf', bbox_inches = 'tight')\n",
    "plot_results_multi_metric(model_type, dataset, hyperparam_rk, metrics, [[94, 96.5]])\n",
    "plt.savefig('rcv1_rk.pdf', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'news20'\n",
    "model_type = get_model_type(dataset)\n",
    "hyperparam_uf = {'name': 'update_freq', 'label': 'Update frequency'}\n",
    "hyperparam_rk = {'name': 'rank', 'label': 'Rank'}\n",
    "\n",
    "# plot_results_multi_metric(model_type, dataset, hyperparam_uf, metrics, [[89, 97], [89, 100], [8e-2, 4e-1], [2e-2, 4e-1]])\n",
    "# plot_results_multi_metric(model_type, dataset, hyperparam_rk, metrics, [[91, 97], [93, 100], [8e-2, 3e-1], [2e-2, 3e-1]])\n",
    "\n",
    "metrics = [{'name': 'test_acc', 'label': 'Test Accuracy (%)'}]\n",
    "plot_results_multi_metric(model_type, dataset, hyperparam_uf, metrics, [[89, 97]])\n",
    "plt.savefig('news20_uf.pdf', bbox_inches = 'tight')\n",
    "plot_results_multi_metric(model_type, dataset, hyperparam_rk, metrics, [[91, 97]])\n",
    "plt.savefig('news20_rk.pdf', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'real-sim'\n",
    "model_type = get_model_type(dataset)\n",
    "hyperparam_uf = {'name': 'update_freq', 'label': 'Update frequency'}\n",
    "hyperparam_rk = {'name': 'rank', 'label': 'Rank'}\n",
    "\n",
    "# plot_results_multi_metric(model_type, dataset, hyperparam_uf, metrics, [[97, 98], [98, 100], [6e-2, 1e-1], [2e-2, 1e-1]])\n",
    "# plot_results_multi_metric(model_type, dataset, hyperparam_rk, metrics, [[96.5, 97.5], [98, 100], [6e-2, 1e-1], [2e-2, 1e-1]])\n",
    "\n",
    "metrics = [{'name': 'test_acc', 'label': 'Test Accuracy (%)'}]\n",
    "plot_results_multi_metric(model_type, dataset, hyperparam_uf, metrics, [[97, 98]])\n",
    "plt.savefig('real-sim_uf.pdf', bbox_inches = 'tight')\n",
    "plot_results_multi_metric(model_type, dataset, hyperparam_rk, metrics, [[96.5, 97.5]])\n",
    "plt.savefig('real-sim_rk.pdf', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'yearmsd'\n",
    "model_type = get_model_type(dataset)\n",
    "hyperparam_uf = {'name': 'update_freq', 'label': 'Update frequency'}\n",
    "hyperparam_rk = {'name': 'rank', 'label': 'Rank'}\n",
    "\n",
    "# metrics = [{'name': 'test_loss', 'label': 'Test Loss'},\n",
    "#            {'name': 'train_loss', 'label': 'Train Loss'}]\n",
    "\n",
    "# plot_results_multi_metric(model_type, dataset, hyperparam_uf, metrics, [[5e1, 1e2], [5e1, 1e2]])\n",
    "# plot_results_multi_metric(model_type, dataset, hyperparam_rk, metrics, [[5e1, 1e2], [5e1, 1e2]])\n",
    "\n",
    "metrics = [{'name': 'test_loss', 'label': 'Test Loss'}]\n",
    "plot_results_multi_metric(model_type, dataset, hyperparam_uf, metrics, [[5e1, 1e2]])\n",
    "plt.savefig('yearmsd_uf.pdf', bbox_inches = 'tight')\n",
    "plot_results_multi_metric(model_type, dataset, hyperparam_rk, metrics, [[5e1, 1e2]])\n",
    "plt.savefig('yearmsd_rk.pdf', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'e2006'\n",
    "model_type = get_model_type(dataset)\n",
    "hyperparam_uf = {'name': 'update_freq', 'label': 'Update frequency'}\n",
    "hyperparam_rk = {'name': 'rank', 'label': 'Rank'}\n",
    "\n",
    "metrics = [{'name': 'test_loss', 'label': 'Test Loss'}]\n",
    "#,{'name': 'train_loss', 'label': 'Train Loss'}]\n",
    "\n",
    "plot_results_multi_metric(model_type, dataset, hyperparam_uf, metrics, [[1.2e-1, 2e-1]])\n",
    "plt.savefig('e2006_uf.pdf', bbox_inches = 'tight')\n",
    "plot_results_multi_metric(model_type, dataset, hyperparam_rk, metrics, [[1.2e-1, 2e-1]])\n",
    "plt.savefig('e2006_rk.pdf', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'w8a'\n",
    "model_type = get_model_type(dataset)\n",
    "hyperparam_uf = {'name': 'update_freq', 'label': 'Update frequency'}\n",
    "hyperparam_rk = {'name': 'rank', 'label': 'Rank'}\n",
    "\n",
    "# metrics = [{'name': 'test_acc', 'label': 'Test Accuracy (%)'},\n",
    "#            {'name': 'train_acc', 'label': 'Train Accuracy (%)'},\n",
    "#            {'name': 'test_loss', 'label': 'Test Loss'},\n",
    "#            {'name': 'train_loss', 'label': 'Train Loss'}]\n",
    "\n",
    "# plot_results_multi_metric(model_type, dataset, hyperparam_uf, metrics, [[97, 99], [97, 99], [2e-2, 4e-2], [2e-2, 4e-2]])\n",
    "# plot_results_multi_metric(model_type, dataset, hyperparam_rk, metrics, [[98, 99], [98, 99], [2e-2, 4e-2], [2e-2, 4e-2]])\n",
    "\n",
    "metrics = [{'name': 'test_acc', 'label': 'Test Accuracy (%)'}]\n",
    "plot_results_multi_metric(model_type, dataset, hyperparam_uf, metrics, [[97, 99]])\n",
    "plt.savefig('w8a_uf.pdf', bbox_inches = 'tight')\n",
    "plot_results_multi_metric(model_type, dataset, hyperparam_rk, metrics, [[97, 99]])\n",
    "plt.savefig('w8a_rk.pdf', bbox_inches = 'tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icml_env",
   "language": "python",
   "name": "icml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
