{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results_opt(model_type, dataset, opt):\n",
    "    results = {}\n",
    "    results_dir = os.path.join('./general_results', model_type, dataset, opt)\n",
    "    filenames = os.listdir(results_dir)\n",
    "    if opt in ['sgd', 'svrg', 'slbfgs', 'lkatyusha']:\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.csv'):\n",
    "                start = filename.find('_') + len('_')\n",
    "                end = filename.find('_seed_')\n",
    "                hyperparam = filename[start:end] # Get lr or L\n",
    "                if hyperparam not in list(results.keys()):\n",
    "                    results[hyperparam] = []\n",
    "                \n",
    "                df = pd.read_csv(os.path.join(results_dir, filename))\n",
    "                results[hyperparam].append(df)\n",
    "    elif opt == 'sketchysgd':\n",
    "        results['auto'] = []\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.csv'):\n",
    "                df = pd.read_csv(os.path.join(results_dir, filename))\n",
    "                results['auto'].append(df)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_averages(opt_results):\n",
    "    averages = {}\n",
    "\n",
    "    for key in opt_results.keys():\n",
    "        averages[key] = {}\n",
    "        n = len(opt_results[key]) # Number of runs for a given hyperparameter\n",
    "        data_cols = list(opt_results[key][0].columns[1:]) # Get column names\n",
    "        for metric in data_cols:\n",
    "            for i in range(n):\n",
    "                metric_data = 1/n * opt_results[key][i][metric].to_numpy()\n",
    "                metric_data_adj = np.nan_to_num(metric_data, nan = np.inf) # When the loss is nan, replace with inf\n",
    "                if metric not in averages[key].keys():\n",
    "                    averages[key][metric] = metric_data_adj\n",
    "                else:\n",
    "                    averages[key][metric] += metric_data_adj\n",
    "\n",
    "    return averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_peak_run(opt_averages, opt_name, metric):\n",
    "    best_run = None\n",
    "    best_hyperparam = None\n",
    "\n",
    "    for key in opt_averages.keys():\n",
    "        if best_run is None:\n",
    "            best_run = opt_averages[key]\n",
    "            if opt_name in ['sgd', 'svrg', 'slbfgs', 'lkatyusha']:\n",
    "                best_hyperparam = key\n",
    "        else:\n",
    "            if 'loss' in metric:\n",
    "                if np.min(opt_averages[key][metric]) < np.min(best_run[metric]):\n",
    "                    best_run = opt_averages[key]\n",
    "                    if opt_name in ['sgd', 'svrg', 'slbfgs', 'lkatyusha']:\n",
    "                        best_hyperparam = key\n",
    "            elif 'acc' in metric:\n",
    "                if np.max(opt_averages[key][metric]) > np.max(best_run[metric]):\n",
    "                    best_run = opt_averages[key]\n",
    "                    if opt_name in ['sgd', 'svrg', 'slbfgs', 'lkatyusha']:\n",
    "                        best_hyperparam = key\n",
    "    \n",
    "    return best_run, best_hyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_metric(avg_results, opts, metric, dataset, colors, lims):\n",
    "    plt.figure()\n",
    "    opt_times = np.zeros(len(opts))\n",
    "    \n",
    "    for i, opt in enumerate(opts):\n",
    "        best_run, best_hyperparam = get_best_peak_run(avg_results[opt], opt, metric['name'])\n",
    "        label = opt\n",
    "        if opt in ['sgd', 'svrg', 'slbfgs']:\n",
    "            print(f\"{opt}: best LR = {float(best_hyperparam):.8f}\")\n",
    "        elif opt == 'lkatyusha':\n",
    "            print(f\"{opt}: best L = {float(best_hyperparam):.8f}\")\n",
    "        \n",
    "        if opt == 'sketchysgd':\n",
    "            linestyle = 'dashed'\n",
    "        else:\n",
    "            linestyle = 'solid'\n",
    "\n",
    "        if 'loss' in metric['name']:\n",
    "            plt.semilogy(np.cumsum(best_run['times']), best_run[metric['name']],\n",
    "                         colors[opt], label = label, linestyle = linestyle)\n",
    "        else:\n",
    "            plt.plot(np.cumsum(best_run['times']), best_run[metric['name']],\n",
    "                     colors[opt], label = label, linestyle = linestyle)\n",
    "    \n",
    "        opt_times[i] = np.cumsum(best_run['times'])[-1]\n",
    "        if opt == 'sketchysgd':\n",
    "            print(best_run['train_loss'][0:100] - best_run['test_loss'][0:100])\n",
    "    opt_times_sorted = np.sort(opt_times)\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel(metric['label'])\n",
    "    plt.title(dataset)\n",
    "    plt.ylim(lims)\n",
    "    plt.xlim(0, opt_times_sorted[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_multi_metric(model_type, dataset, opts, metrics, colors, lims):\n",
    "    avg_results = dict.fromkeys(opts)\n",
    "    for opt in opts:\n",
    "        opt_results = load_results_opt(model_type, dataset, opt)\n",
    "        avg_results[opt] = compute_averages(opt_results)\n",
    "\n",
    "    for metric, lim in zip(metrics, lims):\n",
    "        plot_results_metric(avg_results, opts, metric, dataset, colors, lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_avgs(model_type, dataset, opts, metric, colors, lim):\n",
    "    from matplotlib.lines import Line2D\n",
    "    avg_results = dict.fromkeys(opts)\n",
    "    for opt in opts:\n",
    "        opt_results = load_results_opt(model_type, dataset, opt)\n",
    "        avg_results[opt] = compute_averages(opt_results)\n",
    "\n",
    "    plt.figure()\n",
    "    opt_times = np.zeros(len(opts))\n",
    "    legend_elements = []\n",
    "    for i, opt in enumerate(opts):\n",
    "        best_run, best_hyperparam = get_best_peak_run(avg_results[opt], opt, metric['name'])\n",
    "        if opt in ['sgd', 'svrg', 'slbfgs']:\n",
    "            print(f\"{opt}: best LR = {float(best_hyperparam):.8f}\")\n",
    "        elif opt == 'lkatyusha':\n",
    "            print(f\"{opt}: best L = {float(best_hyperparam):.8f}\")\n",
    "\n",
    "        # Specifications for legend\n",
    "        if opt == 'sketchysgd':\n",
    "            alpha = 1\n",
    "            linestyle = 'dashed'\n",
    "        else:\n",
    "            alpha = 0.2\n",
    "            linestyle = 'solid'\n",
    "        legend_elements.append(Line2D([0], [0], color=colors[opt], label=opt, alpha=alpha, linestyle = linestyle))\n",
    "\n",
    "        opt_time = np.inf\n",
    "        for key in avg_results[opt].keys():\n",
    "            \n",
    "            # If we are plotting something other than sketchysgd that does not correspond to the best hyperparameter, make it translucent\n",
    "            if opt in ['sgd', 'svrg', 'slbfgs', 'lkatyusha'] and key != best_hyperparam:\n",
    "                alpha = 0.2\n",
    "            else:\n",
    "                alpha = 1\n",
    "\n",
    "            if 'loss' in metric['name']:\n",
    "                plt.semilogy(np.cumsum(avg_results[opt][key]['times']), avg_results[opt][key][metric['name']],\n",
    "                             colors[opt], alpha = alpha, linestyle = linestyle)\n",
    "            else:\n",
    "                plt.plot(np.cumsum(avg_results[opt][key]['times']), avg_results[opt][key][metric['name']],\n",
    "                         colors[opt], alpha = alpha, linestyle = linestyle)\n",
    "\n",
    "            # Get the smallest time taken by the optimization algorithm across all hyperparameters\n",
    "            opt_time = min(opt_time, np.cumsum(avg_results[opt][key]['times'])[-1])\n",
    "        opt_times[i] = opt_time\n",
    "    opt_times_sorted = np.sort(opt_times)\n",
    "\n",
    "    plt.legend(handles = legend_elements, bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel(metric['label'])\n",
    "    plt.title(dataset)\n",
    "    plt.ylim(lim)\n",
    "    plt.xlim(0, opt_times_sorted[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_type(dataset):\n",
    "    if dataset in ['rcv1', 'news20', 'real-sim']:\n",
    "        model_type = 'logistic'\n",
    "    elif dataset in ['yearmsd', 'e2006', 'w8a']:\n",
    "        model_type = 'least_squares'\n",
    "\n",
    "    return model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = ['sketchysgd', 'sgd', 'svrg', 'slbfgs', 'lkatyusha']\n",
    "colors = {'sketchysgd': 'k', 'sgd': 'tab:blue', 'svrg': 'tab:orange', 'slbfgs': 'tab:brown', 'lkatyusha': 'tab:olive'}\n",
    "metrics = [{'name': 'test_acc', 'label': 'Test Accuracy (%)'},\n",
    "           {'name': 'train_acc', 'label': 'Train Accuracy (%)'},\n",
    "           {'name': 'test_loss', 'label': 'Test Loss'},\n",
    "           {'name': 'train_loss', 'label': 'Train Loss'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'rcv1'\n",
    "model_type = get_model_type(dataset)\n",
    "plot_avgs(model_type, dataset, opts, metrics[0], colors, [90, 97])\n",
    "\n",
    "plt.savefig('./rcv1_test_acc.pdf', bbox_inches = 'tight')\n",
    "# plot_results_multi_metric(model_type, dataset, opts, metrics, [[92, 97], [92, 100], [1e-1, 2e-1], [2e-2, 2e-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'news20'\n",
    "model_type = get_model_type(dataset)\n",
    "plot_avgs(model_type, dataset, opts, metrics[0], colors, [80, 98])\n",
    "plt.savefig('./news20_test_acc.pdf', bbox_inches = 'tight')\n",
    "# plot_results_multi_metric(model_type, dataset, opts, metrics, [[83, 98], [83, 100], [1e-1, 6e-1], [1e-2, 7e-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'real-sim'\n",
    "model_type = get_model_type(dataset)\n",
    "plot_avgs(model_type, dataset, opts, metrics[0], colors, [90, 98])\n",
    "plt.savefig('./real-sim_test_acc.pdf', bbox_inches = 'tight')\n",
    "# plot_results_multi_metric(model_type, dataset, opts, metrics, [[96, 98], [97, 100], [6e-2, 2e-1], [2e-2, 2e-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'yearmsd'\n",
    "model_type = get_model_type(dataset)\n",
    "\n",
    "metrics = [{'name': 'test_loss', 'label': 'Test Loss'},\n",
    "           {'name': 'train_loss', 'label': 'Train Loss'}]\n",
    "\n",
    "plot_avgs(model_type, dataset, opts, metrics[0], colors, [5e1, 1e4])\n",
    "plt.savefig('./yearmsd_test_loss.pdf', bbox_inches = 'tight')\n",
    "\n",
    "metrics = [{'name': 'train_loss', 'label': 'Train Loss'}]\n",
    "plot_results_multi_metric(model_type, dataset, opts, metrics, colors, [[5e1, 5e3], [5e1, 5e3]])\n",
    "plt.savefig('./yearmsd_train_loss.pdf', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'e2006'\n",
    "model_type = get_model_type(dataset)\n",
    "\n",
    "metrics = [{'name': 'test_loss', 'label': 'Test Loss'},\n",
    "           {'name': 'train_loss', 'label': 'Train Loss'}]\n",
    "\n",
    "plot_avgs(model_type, dataset, opts, metrics[0], colors, [1.2e-1, 7e-1])\n",
    "plt.savefig('./e2006_test_loss.pdf', bbox_inches = 'tight')\n",
    "\n",
    "metrics = [{'name': 'train_loss', 'label': 'Train Loss'}]\n",
    "plot_results_multi_metric(model_type, dataset, opts, metrics, colors, [[1.2e-1, 7e-1], [1.4e-1, 5e-1]])\n",
    "plt.savefig('./e2006_train_loss.pdf', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'w8a'\n",
    "model_type = get_model_type(dataset)\n",
    "\n",
    "metrics = [{'name': 'test_acc', 'label': 'Test Accuracy (%)'},\n",
    "           {'name': 'train_acc', 'label': 'Train Accuracy (%)'},\n",
    "           {'name': 'test_loss', 'label': 'Test Loss'},\n",
    "           {'name': 'train_loss', 'label': 'Train Loss'}]\n",
    "\n",
    "plot_avgs(model_type, dataset, opts, metrics[0], colors, [90, 99])\n",
    "plt.savefig('./w8a_test_acc.pdf', bbox_inches = 'tight')\n",
    "\n",
    "metrics = [{'name': 'train_loss', 'label': 'Train Loss'}]\n",
    "plot_results_multi_metric(model_type, dataset, opts, metrics, colors, [[2e-2, 1e-1]])\n",
    "plt.savefig('./w8a_train_loss.pdf', bbox_inches = 'tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icml_env",
   "language": "python",
   "name": "icml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
