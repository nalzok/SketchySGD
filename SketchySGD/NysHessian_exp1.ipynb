{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de4a00d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ab8aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"stl10_final.csv\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "57ee33cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>27639</th>\n",
       "      <th>27640</th>\n",
       "      <th>27641</th>\n",
       "      <th>27642</th>\n",
       "      <th>27643</th>\n",
       "      <th>27644</th>\n",
       "      <th>27645</th>\n",
       "      <th>27646</th>\n",
       "      <th>27647</th>\n",
       "      <th>27648</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>146</td>\n",
       "      <td>143</td>\n",
       "      <td>110</td>\n",
       "      <td>146</td>\n",
       "      <td>143</td>\n",
       "      <td>110</td>\n",
       "      <td>146</td>\n",
       "      <td>143</td>\n",
       "      <td>110</td>\n",
       "      <td>146</td>\n",
       "      <td>...</td>\n",
       "      <td>138</td>\n",
       "      <td>127</td>\n",
       "      <td>119</td>\n",
       "      <td>147</td>\n",
       "      <td>136</td>\n",
       "      <td>122</td>\n",
       "      <td>138</td>\n",
       "      <td>128</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>129</td>\n",
       "      <td>140</td>\n",
       "      <td>73</td>\n",
       "      <td>124</td>\n",
       "      <td>133</td>\n",
       "      <td>68</td>\n",
       "      <td>138</td>\n",
       "      <td>144</td>\n",
       "      <td>84</td>\n",
       "      <td>144</td>\n",
       "      <td>...</td>\n",
       "      <td>146</td>\n",
       "      <td>128</td>\n",
       "      <td>94</td>\n",
       "      <td>151</td>\n",
       "      <td>130</td>\n",
       "      <td>91</td>\n",
       "      <td>194</td>\n",
       "      <td>164</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>179</td>\n",
       "      <td>223</td>\n",
       "      <td>114</td>\n",
       "      <td>163</td>\n",
       "      <td>203</td>\n",
       "      <td>104</td>\n",
       "      <td>165</td>\n",
       "      <td>191</td>\n",
       "      <td>103</td>\n",
       "      <td>89</td>\n",
       "      <td>...</td>\n",
       "      <td>124</td>\n",
       "      <td>149</td>\n",
       "      <td>128</td>\n",
       "      <td>114</td>\n",
       "      <td>140</td>\n",
       "      <td>117</td>\n",
       "      <td>104</td>\n",
       "      <td>131</td>\n",
       "      <td>109</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>29</td>\n",
       "      <td>16</td>\n",
       "      <td>21</td>\n",
       "      <td>35</td>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>41</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>125</td>\n",
       "      <td>80</td>\n",
       "      <td>18</td>\n",
       "      <td>123</td>\n",
       "      <td>80</td>\n",
       "      <td>18</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>147</td>\n",
       "      <td>130</td>\n",
       "      <td>91</td>\n",
       "      <td>200</td>\n",
       "      <td>189</td>\n",
       "      <td>164</td>\n",
       "      <td>127</td>\n",
       "      <td>112</td>\n",
       "      <td>89</td>\n",
       "      <td>68</td>\n",
       "      <td>...</td>\n",
       "      <td>147</td>\n",
       "      <td>131</td>\n",
       "      <td>89</td>\n",
       "      <td>145</td>\n",
       "      <td>129</td>\n",
       "      <td>93</td>\n",
       "      <td>155</td>\n",
       "      <td>138</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27649 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1      2      3      4      5      6      7      8      9      ...  \\\n",
       "0    146    143    110    146    143    110    146    143    110    146  ...   \n",
       "1    129    140     73    124    133     68    138    144     84    144  ...   \n",
       "2    179    223    114    163    203    104    165    191    103     89  ...   \n",
       "3     14     18     29     16     21     35     14     21     41     12  ...   \n",
       "4    147    130     91    200    189    164    127    112     89     68  ...   \n",
       "\n",
       "   27639  27640  27641  27642  27643  27644  27645  27646  27647  27648  \n",
       "0    138    127    119    147    136    122    138    128     93      1  \n",
       "1    146    128     94    151    130     91    194    164    123      0  \n",
       "2    124    149    128    114    140    117    104    131    109      1  \n",
       "3    125     80     18    123     80     18    120     80     18      0  \n",
       "4    147    131     89    145    129     93    155    138    108      0  \n",
       "\n",
       "[5 rows x 27649 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b0748c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b0a29af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 0:-1].to_numpy()\n",
    "Y = df.iloc[:, -1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e95a063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X / 256.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e17688d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "922cd549",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d76badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "\n",
    "train_data = TrainData(torch.FloatTensor(X), torch.FloatTensor(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d560c355",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91e85f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassification, self).__init__()\n",
    "        # Number of input features is 27648.\n",
    "        self.layer_1 = nn.Linear(27648, 64) \n",
    "        self.layer_2 = nn.Linear(64, 64)\n",
    "        self.layer_out = nn.Linear(64, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bca3de65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassification2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassification2, self).__init__()\n",
    "        # Number of input features is 27648.\n",
    "        self.layer_1 = nn.Linear(27648, 64) \n",
    "        self.layer_2 = nn.Linear(64, 64)\n",
    "        self.layer_out = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5003d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "977e0f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryClassification2(\n",
      "  (layer_1): Linear(in_features=27648, out_features=64, bias=True)\n",
      "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Using optimizer: SGD_Simple\n"
     ]
    }
   ],
   "source": [
    "# model = BinaryClassification()\n",
    "model = BinaryClassification2()\n",
    "model.to(device)\n",
    "print(model)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "optimizer = SGD_Simple(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2396f1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f9f05588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 0.56988 | Acc: 79.882\n",
      "Epoch 002: | Loss: 0.53210 | Acc: 79.941\n",
      "Epoch 003: | Loss: 0.52507 | Acc: 79.907\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for e in range(1, EPOCHS+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c5597a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_product(xs, ys):\n",
    "    \n",
    "    return sum([torch.sum(x * y) for (x, y) in zip(xs, ys)])\n",
    "\n",
    "def normalization(v):\n",
    "    s = group_product(v, v)\n",
    "    s = s**0.5\n",
    "    s = s.cpu().item()\n",
    "    v = [vi / (s + 1e-6) for vi in v]\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "9c3896c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NysHessian():\n",
    "    \n",
    "    def __init__(self, rank, rho):\n",
    "        self.rank = rank\n",
    "        self.rho = rho\n",
    "    \n",
    "    def get_params_grad(self, model):\n",
    "        params = []\n",
    "        grads = []\n",
    "        for param in model.parameters():\n",
    "            if not param.requires_grad:\n",
    "                continue\n",
    "            params.append(param)\n",
    "            grads.append(0. if param.grad is None else param.grad + 0.)\n",
    "        return params, grads\n",
    "    \n",
    "    def update_Hessian(self, data_loader, model, criterion, device):\n",
    "        \n",
    "        params, gradsH = self.get_params_grad(model)\n",
    "        self.size_vec = [p.size() for p in params]\n",
    "        test_matrix = []\n",
    "        hv_matrix = []\n",
    "        \n",
    "        for i in range(self.rank):\n",
    "            \n",
    "            v = [torch.randn(p.size()).to(device) for p in params]\n",
    "            v = normalization(v)\n",
    "            hv_add = [torch.zeros(p.size()).to(device) for p in params]\n",
    "        \n",
    "            # update hessian for full data or only a batch data?\n",
    "            # use only one batch\n",
    "            \n",
    "            for X_batch, y_batch in data_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                model.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch.unsqueeze(1))\n",
    "                loss.backward(create_graph=True)\n",
    "                params, gradsH = self.get_params_grad(model)\n",
    "                hv = torch.autograd.grad(gradsH, params, grad_outputs=v,only_inputs=True,retain_graph=True)\n",
    "                for i in range(len(hv)):\n",
    "                    hv_add[i].data = hv[i].data.add_(hv_add[i].data)    \n",
    "            \n",
    "            for i in range(len(hv_add)):\n",
    "                hv_add[i].data = torch.div(hv_add[i].data, len(data_loader) * 1.0)\n",
    "                hv_add[i].data = hv_add[i].data.add_(v[i].data * torch.tensor(0.1)) \n",
    "                # the shift step should be optimized, \n",
    "                # in numpy was using np.spacing, but not sure the counterpart in pytorch...\n",
    "            \n",
    "            hv_ex = torch.cat([gi.view(-1) for gi in hv_add])\n",
    "            test_ex = torch.cat([gi.view(-1) for gi in v])\n",
    "            \n",
    "            hv_matrix.append(hv_ex)\n",
    "            test_matrix.append(test_ex)\n",
    "        \n",
    "        # stack the groups of params into a very large matrix,\n",
    "        # not sure if there's a way to avoid that....\n",
    "        # but the storage is still the same\n",
    "        \n",
    "        # stack iteration by iteration, try \n",
    "        \n",
    "        hv_matrix_ex = torch.column_stack(hv_matrix)\n",
    "        test_matrix_ex = torch.column_stack(test_matrix)\n",
    "        \n",
    "        C_ex = torch.linalg.cholesky(torch.mm(test_matrix_ex.t(), hv_matrix_ex))\n",
    "        B_ex = torch.linalg.solve_triangular(C_ex, hv_matrix_ex, upper = False, left = False)\n",
    "        U, S, V = torch.linalg.svd(B_ex, full_matrices = False)\n",
    "        self.U = U\n",
    "        self.S = torch.max(torch.square(S) - torch.tensor(0.1), torch.tensor(0.0))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "90e2bb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NysHessianpartial():\n",
    "    \n",
    "    def __init__(self, rank, rho):\n",
    "        self.rank = rank\n",
    "        self.rho = rho\n",
    "    \n",
    "    def get_params_grad(self, model):\n",
    "        params = []\n",
    "        grads = []\n",
    "        for param in model.parameters():\n",
    "            if not param.requires_grad:\n",
    "                continue\n",
    "            params.append(param)\n",
    "            grads.append(0. if param.grad is None else param.grad + 0.)\n",
    "        return params, grads\n",
    "    \n",
    "    def update_Hessian(self, X_batch, y_batch, model, criterion, device):\n",
    "        \n",
    "        shift = 0.001\n",
    "        params, gradsH = self.get_params_grad(model)\n",
    "        self.size_vec = [p.size() for p in params]\n",
    "        test_matrix = []\n",
    "        hv_matrix = []\n",
    "        \n",
    "        for i in range(self.rank):\n",
    "            \n",
    "            v = [torch.randn(p.size()).to(device) for p in params]\n",
    "            v = normalization(v)\n",
    "            hv_add = [torch.zeros(p.size()).to(device) for p in params]\n",
    "        \n",
    "            # update hessian subsample a batch\n",
    "            \n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            model.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch.unsqueeze(1))\n",
    "            loss.backward(create_graph=True)\n",
    "            params, gradsH = self.get_params_grad(model)\n",
    "            hv = torch.autograd.grad(gradsH, params, grad_outputs=v,only_inputs=True,retain_graph=True)\n",
    "            for i in range(len(hv)):\n",
    "                hv_add[i].data = hv[i].data.add_(hv_add[i].data)    \n",
    "                hv_add[i].data = hv_add[i].data.add_(v[i].data * torch.tensor(shift)) \n",
    "                         \n",
    "            hv_ex = torch.cat([gi.view(-1) for gi in hv_add])\n",
    "            test_ex = torch.cat([gi.view(-1) for gi in v])\n",
    "            \n",
    "            hv_matrix.append(hv_ex)\n",
    "            test_matrix.append(test_ex)\n",
    "        \n",
    "        hv_matrix_ex = torch.column_stack(hv_matrix)\n",
    "        test_matrix_ex = torch.column_stack(test_matrix)\n",
    "        choleskytarget = torch.mm(test_matrix_ex.t(), hv_matrix_ex)\n",
    "        \n",
    "        try:\n",
    "            C_ex = torch.linalg.cholesky(choleskytarget)\n",
    "        except:\n",
    "            eigs, eigvectors = torch.linalg.eigh(C_ex)\n",
    "            shift = shift + torch.abs(torch.min(L))\n",
    "            eigs = eigs + shift\n",
    "            C_ex = torch.linalg.cholesky(torch.mm(eigvectors, torch.mm(torch.diag(eigs), eigvectors.T)))\n",
    "\n",
    "        B_ex = torch.linalg.solve_triangular(C_ex, hv_matrix_ex, upper = False, left = False)\n",
    "        U, S, V = torch.linalg.svd(B_ex, full_matrices = False)\n",
    "        self.U = U\n",
    "        self.S = torch.max(torch.square(S) - torch.tensor(shift), torch.tensor(0.0))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12ff082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hidden layer mlp check\n",
    "# experiments, \n",
    "# randomized svd\n",
    "# check kth eigenvalue to be positive, ApproxMinEvec in sketchy cgal paper\n",
    "# when cholesky fails, add to the diagonal to make cholesky work\n",
    "# if cholesky fails, doing eigen-decomposition of the cholesky matrix, figure out the smallest eigenvalue, that's what\n",
    "# we want to add as shift + epsilon\n",
    "# plot landscape and from origin to destination\n",
    "# cifar10, or mnist may have better performance\n",
    "# basic resnet, \n",
    "# smallest network that close to sota structures, concise net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd15fdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nysh = NysHessian(2, 0.1)\n",
    "model.train()\n",
    "nysh.update_Hessian(train_loader, model, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "94e2457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d1220615",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NysHessianOpt(Optimizer):\n",
    "    r\"\"\"Implements NysHessian.\n",
    "    Args:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float): learning rate\n",
    "        rank (int): sketch rank\n",
    "        rho: regularization\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr, rank = 10, rho = 1.0):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "            \n",
    "        defaults = dict(lr=lr, rank = rank, rho = rho)\n",
    "        self.nysh = NysHessian(rank, rho)\n",
    "        super(NysHessianOpt, self).__init__(params, defaults)\n",
    "         \n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            rho = group['rho']\n",
    "            g = torch.cat([p.grad.view(-1) for p in group['params']])\n",
    "            UTg = torch.mv(self.nysh.U.t(), g) \n",
    "            g_new = torch.mv(self.nysh.U, (self.nysh.S + rho).reciprocal() * UTg) + g / rho - torch.mv(self.nysh.U, UTg) / rho            \n",
    "            ls = 0\n",
    "            for p in group['params']:\n",
    "                gp = g_new[ls:ls+torch.numel(p)].view(p.shape)\n",
    "                ls += torch.numel(p)\n",
    "                p.data.add_(-group['lr'] * gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "9e9a8b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryClassification2(\n",
      "  (layer_1): Linear(in_features=27648, out_features=64, bias=True)\n",
      "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# model = BinaryClassification()\n",
    "model = BinaryClassification2()\n",
    "model.to(device)\n",
    "print(model)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "optimizer = NysHessianOpt(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "a0615978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 0.56188 | Acc: 78.265\n",
      "Epoch 002: | Loss: 0.53086 | Acc: 80.064\n",
      "Epoch 003: | Loss: 0.52097 | Acc: 80.103\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for e in range(1, EPOCHS+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    optimizer.nysh.update_Hessian(train_loader, model, criterion, device)\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f71e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Optimizer\n",
    "\n",
    "# simple sgd optimizer, adapted from pytorch official implementation. \n",
    "\n",
    "class SGD_Simple(Optimizer):\n",
    "    \n",
    "    def __init__(self, params, lr, weight_decay=0.0):\n",
    "        \n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "            \n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
    "        \n",
    "        super(SGD_Simple, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = group['weight_decay']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad.data\n",
    "                if weight_decay != 0:\n",
    "                    d_p.add_(weight_decay, p.data)\n",
    "                    \n",
    "                p.data.add_(-group['lr'], d_p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualsz5332",
   "language": "python",
   "name": "virtualsz5332"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
