{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a918b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"volkert.csv\", header = None)\n",
    "\n",
    "X = df.iloc[:, 2:].to_numpy()\n",
    "Y = df.iloc[:, 1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7df189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Optimizer\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc8a00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "\n",
    "full_data = FullData(torch.from_numpy(X).float(), torch.from_numpy(Y).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f91e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(full_data))\n",
    "test_size = len(full_data) - train_size\n",
    "train_data, test_data = torch.utils.data.random_split(full_data, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3bd848",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiClassification, self).__init__()\n",
    "\n",
    "        self.layer_1 = nn.Linear(180, 256) \n",
    "        self.layer_2 = nn.Linear(256, 256)\n",
    "        self.layer_3 = nn.Linear(256, 256)\n",
    "        self.layer_out = nn.Linear(256, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.layer_1(inputs)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.layer_2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.layer_3(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.layer_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6fb03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MultiClassification()\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "torch.save(model.state_dict(), \"volkert_mlp.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816b90b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    \n",
    "    correct_pred = (y_pred_tags == y_test).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "    \n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d688b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_list = [0.01, 0.02, 0.05, 0.08, 0.1, 0.005, 0.002, 0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ab274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_data = []\n",
    "\n",
    "for learning_rate in learning_rate_list:\n",
    "\n",
    "    model = MultiClassification()\n",
    "    model.load_state_dict(torch.load(\"volkert_mlp.pth\"))\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    timelist_sgd = []\n",
    "    losslist_sgd_train = []\n",
    "    acclist_sgd_train = []\n",
    "    losslist_sgd_test = []\n",
    "    acclist_sgd_test = []\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    for e in range(1, 100+1):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(X_batch)\n",
    "\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            acc = multi_acc(y_pred, y_batch)\n",
    "\n",
    "            loss.backward(create_graph = True)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            val_epoch_loss = 0\n",
    "            val_epoch_acc = 0\n",
    "\n",
    "            model.eval()\n",
    "            for X_val_batch, y_val_batch in test_loader:\n",
    "                X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "\n",
    "                y_val_pred = model(X_val_batch)\n",
    "\n",
    "                val_loss = criterion(y_val_pred, y_val_batch)\n",
    "                val_acc = multi_acc(y_val_pred, y_val_batch)\n",
    "\n",
    "                val_epoch_loss += val_loss.item()\n",
    "                val_epoch_acc += val_acc.item()\n",
    "\n",
    "\n",
    "        if e % 50 == 0:\n",
    "            print(f'Epoch {e+0:03}: | train Loss: {epoch_loss/len(train_loader):.5f} | train Acc: {epoch_acc/len(train_loader):.3f}')\n",
    "            print(f'Epoch {e+0:03}: | test Loss: {val_epoch_loss/len(test_loader):.5f} | test Acc: {val_epoch_acc/len(test_loader):.3f}')\n",
    "\n",
    "\n",
    "        epoch_time = timeit.default_timer()\n",
    "        losslist_sgd_train.append(epoch_loss/len(train_loader))\n",
    "        timelist_sgd.append(epoch_time - start_time)\n",
    "        acclist_sgd_train.append(epoch_acc/len(train_loader))\n",
    "        losslist_sgd_test.append(val_epoch_loss/len(test_loader))\n",
    "        acclist_sgd_test.append(val_epoch_acc/len(test_loader))\n",
    "        \n",
    "    sgd_data.append(timelist_sgd)\n",
    "    sgd_data.append(losslist_sgd_train)\n",
    "    sgd_data.append(acclist_sgd_train)\n",
    "    sgd_data.append(losslist_sgd_test)\n",
    "    sgd_data.append(acclist_sgd_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0e61ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_data = []\n",
    "\n",
    "for learning_rate in learning_rate_list:\n",
    "\n",
    "    model = MultiClassification()\n",
    "    model.load_state_dict(torch.load(\"volkert_mlp.pth\"))\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    timelist_adam = []\n",
    "    losslist_adam_train = []\n",
    "    acclist_adam_train = []\n",
    "    losslist_adam_test = []\n",
    "    acclist_adam_test = []\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    for e in range(1, 200+1):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(X_batch)\n",
    "\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            acc = multi_acc(y_pred, y_batch)\n",
    "\n",
    "            loss.backward(create_graph = True)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            val_epoch_loss = 0\n",
    "            val_epoch_acc = 0\n",
    "\n",
    "            model.eval()\n",
    "            for X_val_batch, y_val_batch in test_loader:\n",
    "                X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "\n",
    "                y_val_pred = model(X_val_batch)\n",
    "\n",
    "                val_loss = criterion(y_val_pred, y_val_batch)\n",
    "                val_acc = multi_acc(y_val_pred, y_val_batch)\n",
    "\n",
    "                val_epoch_loss += val_loss.item()\n",
    "                val_epoch_acc += val_acc.item()\n",
    "\n",
    "\n",
    "        if e % 50 == 0:\n",
    "            print(f'Epoch {e+0:03}: | train Loss: {epoch_loss/len(train_loader):.5f} | train Acc: {epoch_acc/len(train_loader):.3f}')\n",
    "            print(f'Epoch {e+0:03}: | test Loss: {val_epoch_loss/len(test_loader):.5f} | test Acc: {val_epoch_acc/len(test_loader):.3f}')\n",
    "\n",
    "\n",
    "        epoch_time = timeit.default_timer()\n",
    "        losslist_adam_train.append(epoch_loss/len(train_loader))\n",
    "        timelist_adam.append(epoch_time - start_time)\n",
    "        acclist_adam_train.append(epoch_acc/len(train_loader))\n",
    "        losslist_adam_test.append(val_epoch_loss/len(test_loader))\n",
    "        acclist_adam_test.append(val_epoch_acc/len(test_loader))\n",
    "        \n",
    "    adam_data.append(timelist_adam)\n",
    "    adam_data.append(losslist_adam_train)\n",
    "    adam_data.append(acclist_adam_train)\n",
    "    adam_data.append(losslist_adam_test)\n",
    "    adam_data.append(acclist_adam_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b266bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_product(xs, ys):\n",
    "    \n",
    "    return sum([torch.sum(x * y) for (x, y) in zip(xs, ys)])\n",
    "\n",
    "def normalization(v):\n",
    "    # normalize a vector\n",
    "    \n",
    "    s = group_product(v, v)\n",
    "    s = s**0.5\n",
    "    s = s.cpu().item()\n",
    "    v = [vi / (s + 1e-6) for vi in v]\n",
    "    return v\n",
    "\n",
    "class NysHessianpartial():\n",
    "    \n",
    "    def __init__(self, rank, rho):\n",
    "        self.rank = rank\n",
    "        # rho is the regularization in Nystrom sketch\n",
    "        self.rho = rho\n",
    "    \n",
    "    def get_params_grad(self, model):\n",
    "        # get parameters and differentiation\n",
    "        params = []\n",
    "        grads = []\n",
    "        for param in model.parameters():\n",
    "            if not param.requires_grad:\n",
    "                continue\n",
    "            params.append(param)\n",
    "            grads.append(0. if param.grad is None else param.grad + 0.)\n",
    "        return params, grads\n",
    "    \n",
    "    def update_Hessian(self, X_batch, y_batch, model, criterion, device):\n",
    "        \n",
    "        shift = 0.001\n",
    "        # get the model parameters and gradients\n",
    "        params, gradsH = self.get_params_grad(model)\n",
    "        # remember the size for each group of parameters\n",
    "        self.size_vec = [p.size() for p in params]\n",
    "        # store random gaussian vector to a matrix\n",
    "        test_matrix = []\n",
    "        # Hessian vector product\n",
    "        hv_matrix = []\n",
    "        \n",
    "        for i in range(self.rank):\n",
    "            # generate gaussian random vector\n",
    "            v = [torch.randn(p.size()).to(device) for p in params]\n",
    "            # normalize\n",
    "            v = normalization(v)\n",
    "            # zero vector to store the shape\n",
    "            hv_add = [torch.zeros(p.size()).to(device) for p in params]\n",
    "        \n",
    "            # update hessian with a subsample batch\n",
    "            \n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            model.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward(create_graph=True)\n",
    "            params, gradsH = self.get_params_grad(model)\n",
    "            # calculate the Hessian vector product\n",
    "            hv = torch.autograd.grad(gradsH, params, grad_outputs=v,only_inputs=True,retain_graph=True)\n",
    "            # add initial shift\n",
    "            for i in range(len(hv)):\n",
    "                hv_add[i].data = hv[i].data.add_(hv_add[i].data)    \n",
    "                hv_add[i].data = hv_add[i].data.add_(v[i].data * torch.tensor(shift)) \n",
    "            \n",
    "            # reshape the Hessian vector product into a long vector\n",
    "            hv_ex = torch.cat([gi.view(-1) for gi in hv_add])\n",
    "            # reshape the random vector into a long vector\n",
    "            test_ex = torch.cat([gi.view(-1) for gi in v])\n",
    "            \n",
    "            # append long vectors into a large matrix\n",
    "            hv_matrix.append(hv_ex)\n",
    "            test_matrix.append(test_ex)\n",
    "        \n",
    "        # assemble the large matrix\n",
    "        hv_matrix_ex = torch.column_stack(hv_matrix)\n",
    "        test_matrix_ex = torch.column_stack(test_matrix)\n",
    "        # calculate Omega^T * A * Omega for Cholesky\n",
    "        choleskytarget = torch.mm(test_matrix_ex.t(), hv_matrix_ex)\n",
    "        # perform Cholesky, if fails, do eigendecomposition\n",
    "        # the new shift is the abs of smallest eigenvalue (negative) plus the original shift\n",
    "        try:\n",
    "            C_ex = torch.linalg.cholesky(choleskytarget)\n",
    "        except:\n",
    "            # eigendecomposition, eigenvalues and eigenvector matrix\n",
    "            eigs, eigvectors = torch.linalg.eigh(choleskytarget)\n",
    "            shift = shift + 1.1 * torch.abs(torch.min(eigs))\n",
    "            # add shift to eigenvalues\n",
    "            eigs = eigs + shift\n",
    "            # put back the matrix for Cholesky by eigenvector * eigenvalues after shift * eigenvector^T \n",
    "            C_ex = torch.linalg.cholesky(torch.mm(eigvectors, torch.mm(torch.diag(eigs), eigvectors.T)))\n",
    "        \n",
    "        # triangular solve\n",
    "        # B_ex = torch.linalg.solve_triangular(C_ex, hv_matrix_ex, upper = False, left = False)\n",
    "        B_ex = torch.triangular_solve(hv_matrix_ex.t(), C_ex.t(), upper = True)\n",
    "        # SVD\n",
    "        # U, S, V = torch.linalg.svd(B_ex, full_matrices = False)\n",
    "        U, S, V = torch.linalg.svd(B_ex[0].t(), full_matrices = False)\n",
    "\n",
    "        self.U = U\n",
    "        self.S = torch.max(torch.square(S) - torch.tensor(shift), torch.tensor(0.0))\n",
    "\n",
    "class NysHessianOpt(Optimizer):\n",
    "    r\"\"\"Implements NysHessian.\n",
    "    Args:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float): learning rate\n",
    "        rank (int): sketch rank\n",
    "        rho: regularization\n",
    "    \"\"\"\n",
    "    def __init__(self, params, rank = 100, rho = 0.1):\n",
    "        # initialize the optimizer    \n",
    "        defaults = dict(rank = rank, rho = rho)\n",
    "        self.nysh = NysHessianpartial(rank, rho)\n",
    "        super(NysHessianOpt, self).__init__(params, defaults)\n",
    "         \n",
    "    def step(self, lr):\n",
    "        # one step update\n",
    "        for group in self.param_groups:\n",
    "            rho = group['rho']\n",
    "            # compute gradient as a long vector\n",
    "            g = torch.cat([p.grad.view(-1) for p in group['params']])\n",
    "            # calculate the search direction by Nystrom sketch and solve\n",
    "            UTg = torch.mv(self.nysh.U.t(), g) \n",
    "            g_new = torch.mv(self.nysh.U, (self.nysh.S + rho).reciprocal() * UTg) + g / rho - torch.mv(self.nysh.U, UTg) / rho            \n",
    "            ls = 0\n",
    "            # update model parameters\n",
    "            for p in group['params']:\n",
    "                gp = g_new[ls:ls+torch.numel(p)].view(p.shape)\n",
    "                ls += torch.numel(p)\n",
    "                p.data.add_(-lr * gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b51abee",
   "metadata": {},
   "outputs": [],
   "source": [
    "hes_interval = 2 * len(train_loader) - 1\n",
    "# update Hessian and Nystrom sketch every couple of steps\n",
    "\n",
    "skechysgd_data = []\n",
    "\n",
    "for learning_rate in learning_rate_list:\n",
    "\n",
    "    model = MultiClassification()\n",
    "    model.load_state_dict(torch.load(\"volkert_mlp.pth\"))\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = NysHessianOpt(model.parameters())\n",
    "\n",
    "    hes_iter = 0\n",
    "\n",
    "    timelist_skechysgd = []\n",
    "    losslist_skechysgd_train = []\n",
    "    acclist_skechysgd_train = []\n",
    "    losslist_skechysgd_test = []\n",
    "    acclist_skechysgd_test = []\n",
    "\n",
    "    lr = torch.tensor(learning_rate)\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "\n",
    "    for e in range(1, 200+1):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "\n",
    "            if hes_iter % hes_interval == 0:\n",
    "                # update Hessian and sketch\n",
    "                optimizer.nysh.update_Hessian(X_batch, y_batch, model, criterion, device)\n",
    "\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(X_batch)\n",
    "\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            acc = multi_acc(y_pred, y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step(lr)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            hes_iter += 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            val_epoch_loss = 0\n",
    "            val_epoch_acc = 0\n",
    "\n",
    "            model.eval()\n",
    "            for X_val_batch, y_val_batch in test_loader:\n",
    "                X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "\n",
    "                y_val_pred = model(X_val_batch)\n",
    "\n",
    "                val_loss = criterion(y_val_pred, y_val_batch)\n",
    "                val_acc = multi_acc(y_val_pred, y_val_batch)\n",
    "\n",
    "                val_epoch_loss += val_loss.item()\n",
    "                val_epoch_acc += val_acc.item()\n",
    "\n",
    "\n",
    "        if e % 50 == 0:\n",
    "            print(f'Epoch {e+0:03}: | train Loss: {epoch_loss/len(train_loader):.5f} | train Acc: {epoch_acc/len(train_loader):.3f}')\n",
    "            print(f'Epoch {e+0:03}: | test Loss: {val_epoch_loss/len(test_loader):.5f} | test Acc: {val_epoch_acc/len(test_loader):.3f}')\n",
    "\n",
    "\n",
    "        epoch_time = timeit.default_timer()\n",
    "        losslist_skechysgd_train.append(epoch_loss/len(train_loader))\n",
    "        timelist_skechysgd.append(epoch_time - start_time)\n",
    "        acclist_skechysgd_train.append(epoch_acc/len(train_loader))\n",
    "        losslist_skechysgd_test.append(val_epoch_loss/len(test_loader))\n",
    "        acclist_skechysgd_test.append(val_epoch_acc/len(test_loader))\n",
    "    \n",
    "    skechysgd_data.append(timelist_skechysgd)\n",
    "    skechysgd_data.append(losslist_skechysgd_train)\n",
    "    skechysgd_data.append(acclist_skechysgd_train)\n",
    "    skechysgd_data.append(losslist_skechysgd_test)\n",
    "    skechysgd_data.append(acclist_skechysgd_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7946dffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571cc6c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edc0da2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aac88d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
